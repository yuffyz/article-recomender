{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Article Recommendation Engine\n",
    "\n",
    "\n",
    "(This notebook is mainly for testing purpose)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This program is a simple article recommendation engine built on word2vec. The program takes an input of one article, and return a list of the 5 top article recommendations based on euclidean distance. \n",
    "\n",
    "\n",
    "## How the program works\n",
    "\n",
    "The similarity and relationships between articles are modeled based on word vector. I tested with both the 300 dimension Stanford [GloVe](https://nlp.stanford.edu/projects/glove/) trained on 2014 Wikipedia and the a context free model [BERT](https://github.com/google-research/bert). \n",
    "\n",
    "For GloVe, each word is represented as a vector of 300-floating point mumbers. This vector captures the meaning of the word regarding other words within its pretrained corpus (the 2014 Wikipedia corpus), and is learned from a neural network that captures the word-word co-occurrence probabilities among the 300-d space. \n",
    "\n",
    "For each document, we calculate the centroid of the document's cloud of word vectors by dividing the sum of the vectors by the number of words in the article. The distances between each articles are measured by the euclidean distance between their centroid. \n",
    "\n",
    "\n",
    "## How to run the program \n",
    "\n",
    "The program reads in a database of word vectors and a corpus of text articles then organizing them into a handy table (list of lists) for processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "from sklearn.feature_extraction import stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From scikit learn that got words from:\n",
    "# http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\n",
    "\n",
    "ENGLISH_STOP_WORDS = frozenset([\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(filename):\n",
    "    \"\"\"\n",
    "    Read all lines from the indicated file and return a dictionary\n",
    "    mapping word:vector where vectors are of numpy `array` type.\n",
    "    GloVe file lines are of the form:\n",
    "\n",
    "    the 0.418 0.24968 -0.41242 0.1217 ...\n",
    "\n",
    "    So split each line on spaces into a list; the first element is the word\n",
    "    and the remaining elements represent factor components. The length of the vector\n",
    "    should not matter; read vectors of any length.\n",
    "    \"\"\"\n",
    "    f = open(filename)\n",
    "    contents = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    dictionary = {}\n",
    "    for line in contents:\n",
    "        word = line.rstrip().split(\" \")\n",
    "        dictionary[word[0]] = np.array(word[1:], dtype=\"float\")\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': array([ 0.418  ,  0.24968, -0.41242,  0.1217 ])}\n",
      "{'the': array([ 0.418  ,  0.24968, -0.41242,  0.1217 ]), 'wikipedia': array([-0.67679, -0.11589, -0.22071, -0.22887])}\n"
     ]
    }
   ],
   "source": [
    "contents = [\"the 0.418 0.24968 -0.41242 0.1217\", \"wikipedia -0.67679 -0.11589 -0.22071 -0.22887\"]\n",
    "\n",
    "dictionary = {}\n",
    "for line in range(len(contents)):\n",
    "    word = contents[line].split()\n",
    "    dictionary[word[0]] = np.array(word[1:], dtype=\"float\")\n",
    "    print(dictionary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is over 1GB\n",
    "\n",
    "gloves = load_glove('word2vec/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gloves['wikipedia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filelist(root):\n",
    "    \"\"\"Return a fully-qualified list of filenames under root directory\"\"\"\n",
    "    allfiles = []\n",
    "    for path, subdirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            allfiles.append(os.path.join(path, name))\n",
    "    return allfiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/bbc/.DS_Store',\n",
       " 'data/bbc/COPYRIGHT',\n",
       " 'data/bbc/entertainment/289.txt',\n",
       " 'data/bbc/entertainment/262.txt',\n",
       " 'data/bbc/entertainment/276.txt']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allfiles = filelist('data/bbc')\n",
    "allfiles[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text(filename):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ocean\\'s Twelve raids box office\\n\\nOcean\\'s Twelve, the crime caper sequel starring George Clooney, Brad Pitt and Julia Roberts, has gone straight to number one in the US box office chart.\\n\\nIt took $40.8m (Â£21m) in weekend ticket sales, according to studio estimates. The sequel follows the master criminals as they try to pull off three major heists across Europe. It knocked last week\\'s number one, National Treasure, into third place. Wesley Snipes\\' Blade: Trinity was in second, taking $16.1m (Â£8.4m). Rounding out the top five was animated fable The Polar Express, starring Tom Hanks, and festive comedy Christmas with the Kranks.\\n\\nOcean\\'s Twelve box office triumph marks the fourth-biggest opening for a December release in the US, after the three films in the Lord of the Rings trilogy. The sequel narrowly beat its 2001 predecessor, Ocean\\'s Eleven which took $38.1m (Â£19.8m) on its opening weekend and $184m (Â£95.8m) in total. A remake of the 1960s film, starring Frank Sinatra and the Rat Pack, Ocean\\'s Eleven was directed by Oscar-winning director Steven Soderbergh. Soderbergh returns to direct the hit sequel which reunites Clooney, Pitt and Roberts with Matt Damon, Andy Garcia and Elliott Gould. Catherine Zeta-Jones joins the all-star cast. \"It\\'s just a fun, good holiday movie,\" said Dan Fellman, president of distribution at Warner Bros. However, US critics were less complimentary about the $110m (Â£57.2m) project, with the Los Angeles Times labelling it a \"dispiriting vanity project\". A milder review in the New York Times dubbed the sequel \"unabashedly trivial\".\\n'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = get_text(allfiles[6])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def words(text):\n",
    "    clean_text = text.lower()\n",
    "    print (\"[lower]\", clean_text)\n",
    "\n",
    "    clean_text = re.sub('[' + string.punctuation + '0-9\\\\r\\\\t\\\\n]', ' ', text)\n",
    "    print (\"[regex]\", clean_text)\n",
    "\n",
    "    clean_text = clean_text.split(\" \")\n",
    "    print (\"[split on space]\", clean_text)\n",
    "    \n",
    "    clean_text = [w for w in clean_text if len(w) > 2]\n",
    "    print (\"[ignore words < 3]\", clean_text)\n",
    "    \n",
    "    clean_text = [w for w in clean_text if not w in stop_words.ENGLISH_STOP_WORDS]\n",
    "#     print (\"[stop words]\", clean_text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "# wlist = words(s)\n",
    "# wlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ocean',\n",
       " 'Twelve',\n",
       " 'raids',\n",
       " 'box',\n",
       " 'office',\n",
       " 'Ocean',\n",
       " 'Twelve',\n",
       " 'crime',\n",
       " 'caper',\n",
       " 'sequel',\n",
       " 'starring',\n",
       " 'George',\n",
       " 'Clooney',\n",
       " 'Brad',\n",
       " 'Pitt',\n",
       " 'Julia',\n",
       " 'Roberts',\n",
       " 'gone',\n",
       " 'straight',\n",
       " 'number',\n",
       " 'box',\n",
       " 'office',\n",
       " 'chart',\n",
       " 'took',\n",
       " 'weekend',\n",
       " 'ticket',\n",
       " 'sales',\n",
       " 'according',\n",
       " 'studio',\n",
       " 'estimates',\n",
       " 'The',\n",
       " 'sequel',\n",
       " 'follows',\n",
       " 'master',\n",
       " 'criminals',\n",
       " 'try',\n",
       " 'pull',\n",
       " 'major',\n",
       " 'heists',\n",
       " 'Europe',\n",
       " 'knocked',\n",
       " 'week',\n",
       " 'number',\n",
       " 'National',\n",
       " 'Treasure',\n",
       " 'place',\n",
       " 'Wesley',\n",
       " 'Snipes',\n",
       " 'Blade',\n",
       " 'Trinity',\n",
       " 'second',\n",
       " 'taking',\n",
       " 'Rounding',\n",
       " 'animated',\n",
       " 'fable',\n",
       " 'The',\n",
       " 'Polar',\n",
       " 'Express',\n",
       " 'starring',\n",
       " 'Tom',\n",
       " 'Hanks',\n",
       " 'festive',\n",
       " 'comedy',\n",
       " 'Christmas',\n",
       " 'Kranks',\n",
       " 'Ocean',\n",
       " 'Twelve',\n",
       " 'box',\n",
       " 'office',\n",
       " 'triumph',\n",
       " 'marks',\n",
       " 'fourth',\n",
       " 'biggest',\n",
       " 'opening',\n",
       " 'December',\n",
       " 'release',\n",
       " 'films',\n",
       " 'Lord',\n",
       " 'Rings',\n",
       " 'trilogy',\n",
       " 'The',\n",
       " 'sequel',\n",
       " 'narrowly',\n",
       " 'beat',\n",
       " 'predecessor',\n",
       " 'Ocean',\n",
       " 'Eleven',\n",
       " 'took',\n",
       " 'opening',\n",
       " 'weekend',\n",
       " 'total',\n",
       " 'remake',\n",
       " 'film',\n",
       " 'starring',\n",
       " 'Frank',\n",
       " 'Sinatra',\n",
       " 'Rat',\n",
       " 'Pack',\n",
       " 'Ocean',\n",
       " 'Eleven',\n",
       " 'directed',\n",
       " 'Oscar',\n",
       " 'winning',\n",
       " 'director',\n",
       " 'Steven',\n",
       " 'Soderbergh',\n",
       " 'Soderbergh',\n",
       " 'returns',\n",
       " 'direct',\n",
       " 'hit',\n",
       " 'sequel',\n",
       " 'reunites',\n",
       " 'Clooney',\n",
       " 'Pitt',\n",
       " 'Roberts',\n",
       " 'Matt',\n",
       " 'Damon',\n",
       " 'Andy',\n",
       " 'Garcia',\n",
       " 'Elliott',\n",
       " 'Gould',\n",
       " 'Catherine',\n",
       " 'Zeta',\n",
       " 'Jones',\n",
       " 'joins',\n",
       " 'star',\n",
       " 'cast',\n",
       " 'just',\n",
       " 'fun',\n",
       " 'good',\n",
       " 'holiday',\n",
       " 'movie',\n",
       " 'said',\n",
       " 'Dan',\n",
       " 'Fellman',\n",
       " 'president',\n",
       " 'distribution',\n",
       " 'Warner',\n",
       " 'Bros',\n",
       " 'However',\n",
       " 'critics',\n",
       " 'complimentary',\n",
       " 'project',\n",
       " 'Los',\n",
       " 'Angeles',\n",
       " 'Times',\n",
       " 'labelling',\n",
       " 'dispiriting',\n",
       " 'vanity',\n",
       " 'project',\n",
       " 'milder',\n",
       " 'review',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Times',\n",
       " 'dubbed',\n",
       " 'sequel',\n",
       " 'unabashedly',\n",
       " 'trivial']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a function to clean the texts\n",
    "\n",
    "def words(text):\n",
    "    \"\"\"\n",
    "    Given a string, return a list of words normalized as follows.\n",
    "    Split the string to make words first by using regex compile() function\n",
    "    and string.punctuation + '0-9\\\\r\\\\t\\\\n]' to replace all those\n",
    "    char with a space character.\n",
    "    Split on space to get word list.\n",
    "    Ignore words < 3 char long.\n",
    "    Lowercase all words\n",
    "    Remove English stop words\n",
    "    \"\"\"\n",
    "    clean_text = text.lower()\n",
    "#     print (\"[lower]\", clean_text)\n",
    "\n",
    "    clean_text = re.sub('[' + string.punctuation + '0-9\\\\r\\\\t\\\\n]', ' ', text)\n",
    "#     print \"[regex]\", clean_text\n",
    "\n",
    "    clean_text = clean_text.split(\" \")\n",
    "#     print \"[split]\", clean_text\n",
    "    \n",
    "    clean_text = [w for w in clean_text if len(w) > 2]\n",
    "#     print (\"[ignore words < 3]\", clean_text)\n",
    "\n",
    "    clean_text = [w for w in clean_text if not w in ENGLISH_STOP_WORDS]\n",
    "    return clean_text\n",
    "\n",
    "wlist = words(s)\n",
    "wlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function to calculate the centroid \n",
    "\n",
    "def doc2vec(text, gloves):\n",
    "    \"\"\"\n",
    "    Return the word vector centroid for the text. Sum the word vectors\n",
    "    for each word and then divide by the number of words. Ignore words\n",
    "    not in gloves.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get clean word list \n",
    "    wlist = words(text)\n",
    "    \n",
    "    # we only have word vectors for the words in gloves \n",
    "    ingloves = [w for w in wlist if w in gloves]\n",
    "    \n",
    "    vcorpus = np.zeros(shape=(300,))\n",
    "    for w in ingloves:\n",
    "        vcorpus += gloves[w]\n",
    "        \n",
    "    # the number of words in only the words in gloves \n",
    "    centroid = vcorpus/len(ingloves)\n",
    "    return centroid\n",
    "\n",
    "centroid = doc2vec(s, gloves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.25879, 0.13379000000000002, -0.63313, -0.10716999999999999]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingloves = {'the': [ 0.418  ,  0.24968, -0.41242,  0.1217 ], 'wikipedia': [-0.67679, -0.11589, -0.22071, -0.22887]}\n",
    "\n",
    "vcorpus = []\n",
    "for w in ingloves:\n",
    "        vcorpus.append(ingloves[w])\n",
    "        \n",
    "vcorpus = np.array(vcorpus)\n",
    "vcorpus = vcorpus.transpose()\n",
    "svcorpus = [sum(i) for i in vcorpus]\n",
    "svcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['entertainment/074.txt',\n",
       " \"Ocean's Twelve raids box office\",\n",
       " '\\nOcean\\'s Twelve, the crime caper sequel starring George Clooney, Brad Pitt and Julia Roberts, has gone straight to number one in the US box office chart.\\n\\nIt took $40.8m (Â£21m) in weekend ticket sales, according to studio estimates. The sequel follows the master criminals as they try to pull off three major heists across Europe. It knocked last week\\'s number one, National Treasure, into third place. Wesley Snipes\\' Blade: Trinity was in second, taking $16.1m (Â£8.4m). Rounding out the top five was animated fable The Polar Express, starring Tom Hanks, and festive comedy Christmas with the Kranks.\\n\\nOcean\\'s Twelve box office triumph marks the fourth-biggest opening for a December release in the US, after the three films in the Lord of the Rings trilogy. The sequel narrowly beat its 2001 predecessor, Ocean\\'s Eleven which took $38.1m (Â£19.8m) on its opening weekend and $184m (Â£95.8m) in total. A remake of the 1960s film, starring Frank Sinatra and the Rat Pack, Ocean\\'s Eleven was directed by Oscar-winning director Steven Soderbergh. Soderbergh returns to direct the hit sequel which reunites Clooney, Pitt and Roberts with Matt Damon, Andy Garcia and Elliott Gould. Catherine Zeta-Jones joins the all-star cast. \"It\\'s just a fun, good holiday movie,\" said Dan Fellman, president of distribution at Warner Bros. However, US critics were less complimentary about the $110m (Â£57.2m) project, with the Los Angeles Times labelling it a \"dispiriting vanity project\". A milder review in the New York Times dubbed the sequel \"unabashedly trivial\".\\n',\n",
       " array([-2.51997141e-02,  8.52788109e-02,  6.01865641e-02, -2.54727957e-02,\n",
       "        -2.13810696e-02,  1.19802262e-01, -2.12332077e-01,  6.48450185e-02,\n",
       "        -8.63580011e-02, -7.22292725e-01,  1.58642838e-01,  4.85098120e-02,\n",
       "        -6.14143435e-02,  1.32326737e-01, -6.68249783e-03, -1.54449043e-01,\n",
       "        -1.56989326e-01,  5.32683500e-02,  6.61240720e-02,  7.57301630e-02,\n",
       "         5.88360641e-02,  1.17951196e-01,  6.87627130e-02,  7.90954163e-02,\n",
       "         3.14668054e-02,  1.53063267e-01,  5.31102959e-02, -1.54530736e-01,\n",
       "         1.18655589e-01,  5.59463076e-02, -1.89189450e-01,  1.14269149e-01,\n",
       "         1.54208228e-02,  7.94888467e-02, -7.74247935e-01,  9.42716304e-04,\n",
       "        -1.23757935e-01,  6.14698196e-02, -3.34477717e-03,  7.61460000e-02,\n",
       "         4.58024739e-02, -6.18729609e-02, -6.49984598e-02,  1.54960447e-01,\n",
       "        -5.09597391e-03,  1.32488413e-02,  2.32455370e-02, -6.32835761e-02,\n",
       "        -9.37914054e-02,  1.84117068e-02, -6.06873772e-02, -1.64047867e-01,\n",
       "        -7.30820435e-03,  1.40014659e-01,  1.42232893e-02,  6.05502130e-02,\n",
       "        -1.10414577e-01,  7.72362739e-02,  8.96546451e-02, -1.04366096e-01,\n",
       "        -1.27344043e-01,  6.48494398e-02,  1.00455252e-01, -2.56410446e-02,\n",
       "         8.77501370e-02, -1.64033717e-01,  6.47153772e-02,  2.63867391e-04,\n",
       "         1.80863984e-01, -5.72108696e-05,  3.96064804e-02,  1.15891685e-01,\n",
       "         1.61896073e-01,  1.33163304e-01,  1.23564120e-01, -2.32111141e-02,\n",
       "         4.47889163e-02, -4.27758565e-02,  1.28864348e-03, -5.19034913e-02,\n",
       "        -6.37480620e-02, -9.71992391e-03,  1.99098543e-01,  5.00173870e-02,\n",
       "         1.32356067e-01,  4.79216130e-02,  5.50578587e-03,  7.85501163e-02,\n",
       "         8.15882174e-02, -1.36100728e-01, -1.22839130e-02,  8.61350043e-02,\n",
       "        -3.20819185e-02, -1.40690880e-01, -4.01142220e-02,  3.02553348e-02,\n",
       "        -1.55803354e-01,  1.04175411e-01, -8.51010272e-02, -4.15028902e-01,\n",
       "         5.33353163e-02, -4.34652326e-02,  6.04431646e-02,  3.87472196e-02,\n",
       "         1.25041924e-02, -5.95414250e-02, -8.60955630e-02, -2.35451217e-02,\n",
       "         6.50647935e-03, -1.42963519e-01,  9.48241304e-03, -2.63818880e-02,\n",
       "        -1.01652604e-01, -2.98631870e-02,  1.47345693e-01,  2.76273338e-02,\n",
       "        -1.34296551e-01, -6.75344575e-02, -6.59997533e-02, -3.35546134e-01,\n",
       "        -2.61165587e-02, -3.16689978e-02, -1.06576370e-02,  6.10861424e-02,\n",
       "        -3.58771739e-04,  3.62065185e-02,  8.80599585e-02,  8.82489565e-02,\n",
       "        -8.78497457e-02,  1.07523040e-01, -4.11066509e-02,  1.39903925e-01,\n",
       "         2.41046750e-02, -2.74664500e-02,  1.04383772e-02, -4.80082228e-02,\n",
       "        -5.31244043e-02, -4.90066702e-02, -8.61349643e-02,  1.29364401e-01,\n",
       "        -2.30944120e-02,  8.68454130e-03, -6.32239804e-02,  3.61633043e-03,\n",
       "        -1.29444970e-01, -1.38270938e-01,  4.43238478e-03,  4.20532761e-02,\n",
       "        -3.56214859e-03,  9.60046848e-03,  3.47156473e-01, -2.70920011e-02,\n",
       "        -4.41640685e-02, -9.78304142e-02,  2.24773379e-01, -4.08361848e-03,\n",
       "        -6.47397777e-02,  7.57886380e-02, -9.71635393e-02, -3.56308962e-02,\n",
       "         1.46263682e-01, -9.03459478e-02,  4.64845000e-02, -4.83425124e-02,\n",
       "         2.23208815e-02,  2.47882693e-01, -1.86401424e-02,  8.42415618e-03,\n",
       "        -1.98073337e-02,  1.62488163e-01,  5.95835217e-03, -6.85659043e-02,\n",
       "        -5.64087880e-01, -4.34482446e-02,  4.57150874e-02,  1.13720847e-01,\n",
       "         4.72373043e-03,  1.35254283e-02, -9.07820243e-02,  1.84576696e-02,\n",
       "         4.59020873e-02,  6.37827707e-02,  8.77471998e-02,  6.71298423e-02,\n",
       "         8.84508609e-02, -5.63838252e-02, -7.48881957e-02, -2.76914239e-02,\n",
       "        -7.56993880e-02,  2.89854470e-02, -1.02435409e-01,  8.47297826e-03,\n",
       "        -1.42487458e-02, -6.45645207e-02, -8.69092466e-02, -5.66387554e-02,\n",
       "        -1.80820010e-01,  1.71005435e-03, -1.68154483e-02, -2.65499663e-02,\n",
       "         8.39866772e-01, -4.81943739e-02, -1.58762511e-02, -6.00676989e-02,\n",
       "         7.27106076e-02, -2.26532609e-04, -9.07860359e-02,  4.56871957e-02,\n",
       "        -1.00325493e-01, -1.08866686e-01, -1.47075995e-01, -3.24902293e-02,\n",
       "         8.26448380e-02,  6.36687652e-02,  9.68619457e-02, -1.46155870e-04,\n",
       "         6.61391284e-02,  9.89454446e-02,  1.25566354e-01,  1.26134413e-02,\n",
       "         4.73495424e-02, -3.05058312e-02, -1.73889562e-01,  9.80074576e-02,\n",
       "        -1.13358068e-01, -1.37000551e-01,  1.70993717e-01, -8.99108428e-02,\n",
       "        -5.24583778e-02, -2.45750978e-02,  9.22491848e-03, -4.31711967e-02,\n",
       "        -1.85466861e-01, -1.76452316e-01,  1.65151259e-01,  6.53942283e-02,\n",
       "         1.04176710e-01,  5.29976967e-02,  1.04038315e-02,  1.26365489e-02,\n",
       "         2.17627174e-02, -2.37032739e-02,  9.75543478e-03, -3.81254303e-02,\n",
       "        -2.81828200e-01, -8.57033826e-02,  6.02806109e-02, -4.72537717e-02,\n",
       "        -1.95970772e-02,  9.98649652e-02,  6.25312446e-02, -9.10147228e-02,\n",
       "        -3.76300543e-02,  1.85458832e-01,  1.62756842e-01, -1.96979257e-01,\n",
       "         8.22639062e-02, -8.70485337e-02,  1.37406903e-01,  1.36014268e-01,\n",
       "        -1.75939804e-02, -1.37486652e-02,  8.71351783e-02, -2.22486696e-02,\n",
       "         2.72661685e-02, -1.08279000e-02,  2.99283533e-02, -1.38050082e-01,\n",
       "         2.06093955e-01,  1.35026252e-01,  1.29069548e-01, -2.97695261e-02,\n",
       "         2.39994174e-02,  2.76576761e-02,  7.09720804e-02, -7.83932717e-03,\n",
       "        -1.25621134e+00, -6.77419022e-03,  1.49530424e-01, -4.84305935e-02,\n",
       "        -1.22703185e-01, -4.07082826e-02, -4.57333685e-02,  1.25855576e-02,\n",
       "        -7.36912359e-02,  1.56267738e-01, -5.24841000e-02, -4.22643913e-03,\n",
       "        -3.78519141e-02,  1.15560540e-01, -1.10901148e-01,  2.48186728e-02,\n",
       "        -9.38979652e-02,  1.33304013e-01, -7.03512413e-02,  2.46780364e-01,\n",
       "        -1.20658268e-01, -2.12995217e-02, -3.56885473e-02,  2.90882348e-02])]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_articles(articles_dirname, gloves):\n",
    "    \"\"\"\n",
    "    Load all .txt files under articles_dirname and return a table (list of lists/tuples)\n",
    "    where each record is a list of:\n",
    "\n",
    "      [filename, title, article-text-minus-title, wordvec-centroid-for-article-text]\n",
    "\n",
    "    We use gloves parameter to compute the word vectors and centroid.\n",
    "\n",
    "    The filename is stripped of the prefix of the articles_dirname pulled in as\n",
    "    script parameter sys.argv[2]. E.g., filename will be \"business/223.txt\"\n",
    "    \"\"\"\n",
    "    allfiles = filelist(articles_dirname)\n",
    "    table = []\n",
    "    for f in allfiles:\n",
    "        fname = f[len(articles_dirname)+1:]\n",
    "        \n",
    "        # split the title and article by splitting at \\n\n",
    "        # and then join the article again \n",
    "        text = get_text(f)\n",
    "        lines = text.split('\\n')\n",
    "        title = lines[0]\n",
    "        \n",
    "        article = lines[1:]\n",
    "        article = \"\\n\".join(article)\n",
    "        \n",
    "        centriold = doc2vec(text, gloves)\n",
    "        table.append([fname, title, article, centriold])\n",
    "    return table \n",
    "\n",
    "table = load_articles('data/bbc', gloves)\n",
    "table[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distances(article, articles):\n",
    "    \"\"\"\n",
    "    Compute the euclidean distance from article to every other article and return\n",
    "    a list of (distance, a) tuples for all a in articles. The article is one\n",
    "    of the elements (tuple) from the articles list.\n",
    "    \"\"\"\n",
    "    dist = []\n",
    "    for a in articles:\n",
    "        if a != article:\n",
    "            distance = np.linalg.norm(a[3] - article[3])\n",
    "            adist = (distance, a)\n",
    "            dist.append(adist)\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "eucli_dis = distances(table[10], table)\n",
    "eucli_dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommended(article, articles, n):\n",
    "    \"\"\"\n",
    "    Return a list of the n articles (records with filename, title, etc...)\n",
    "    closest to article's word vector centroid. The article is one of the elements\n",
    "    (tuple) from the articles list.\n",
    "    \"\"\"\n",
    "    dist = distances(article, articles)\n",
    "    rank = sorted(dist, key=lambda x:x[0])[0:n]\n",
    "    rec = [a[1] for a in rank]\n",
    "    return rec\n",
    "\n",
    "rec = recommended(table[5], table, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 4), (3, 3)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [(1,2), (3,3), (2,4)]\n",
    "b = sorted(a, key=lambda x: x[0])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
